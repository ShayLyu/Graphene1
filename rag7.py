import os
import json
import streamlit as st
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv
from typing import List
from datetime import datetime
import requests

# å¦‚æœä½ ä½¿ç”¨ Qwen æ¥å£ï¼Œéœ€è¦å®‰è£…å¹¶å¼•å…¥ openai åŒ… (pip install openai)
# è¿™é‡Œ import OpenAI ä»…ä½œç¤ºä¾‹ï¼Œè¯·æ ¹æ®å®é™…éœ€æ±‚ä¿®æ”¹
from openai import OpenAI
from langchain.embeddings.base import Embeddings
from langchain.callbacks.base import BaseCallbackHandler


########################################################################################
# 1. è‡ªå®šä¹‰ Embeddings ç±» (QwenEmbeddings)ï¼Œç”¨äºè°ƒç”¨ DashScope / é˜¿é‡Œäº‘ Qwen Embeddings
########################################################################################

class QwenEmbeddings(Embeddings):
    def __init__(
            self,
            api_key: str = None,
            model: str = "text-embedding-v3",
            base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
            dimensions: int = 1024
    ):
        self.client = OpenAI(
            api_key=api_key or os.environ.get('DASHSCOPE_API_KEY'),
            base_url=base_url,
        )
        self.model = model
        self.dimensions = dimensions

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡"""
        try:
            texts = [str(text) for text in texts]
            response = self.client.embeddings.create(
                model=self.model,
                input=texts,
                dimensions=self.dimensions,
                encoding_format="float"
            )
            return [item.embedding for item in response.data]
        except Exception as e:
            print(f"Embedding error in embed_documents: {e}")
            raise

    def embed_query(self, text: str) -> List[float]:
        """å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡"""
        try:
            text = str(text)
            response = self.client.embeddings.create(
                model=self.model,
                input=[text],
                dimensions=self.dimensions,
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"Embedding error in embed_query: {e}")
            raise


########################################################################################
# 2. è‡ªå®šä¹‰ Streamlit å›è°ƒï¼Œç”¨äºå¤§æ¨¡å‹å›ç­”æ—¶æµå¼è¾“å‡º
########################################################################################

class StreamlitCallbackHandler(BaseCallbackHandler):
    """
    å°†å¤§æ¨¡å‹çš„æµå¼è¾“å‡ºå®æ—¶æ˜¾ç¤ºåˆ° Streamlit ç•Œé¢çš„å›è°ƒç±»ã€‚
    """

    def __init__(self, container):
        # container å¯ä»¥æ˜¯ st.empty()ã€st.container() ç­‰
        self.container = container
        # ç”¨æ¥ç¼“å­˜å½“å‰ç´¯è®¡çš„æ–‡æœ¬
        self.current_text = ""
        self.initialized = False  # æ ‡è®°æ˜¯å¦å·²å†™å…¥æ ‡é¢˜

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """
        å½“ LLM ç”Ÿæˆæ–°çš„ token æ—¶ï¼Œä¼šè°ƒç”¨è¯¥æ–¹æ³•ã€‚
        """
        if not self.initialized:
            # å†™å…¥æ ‡é¢˜
            self.container.markdown("### ğŸ¤– æ™ºèƒ½å›å¤")
            self.initialized = True
        self.current_text += token
        # å°†æœ€æ–°ç´¯è®¡çš„æ–‡æœ¬å®æ—¶æ›´æ–°åˆ°å‰ç«¯
        self.container.markdown(self.current_text)


########################################################################################
# 3. å®šä¹‰ä¸»ç±» PDFKnowledgeBaseQAï¼Œç”¨äºçŸ¥è¯†åº“é—®ç­”
########################################################################################

class PDFKnowledgeBaseQA:
    def __init__(
            self,
            knowledge_base_path: str,
            model: str = 'qwen-max',
            embedding_model: str = 'text-embedding-v3',
            base_url: str = 'https://dashscope.aliyuncs.com/compatible-mode/v1',
            temperature: float = 0.7,
            top_p: float = 0.7,
    ):
        # åŠ è½½ .env ä¸­çš„ç¯å¢ƒå˜é‡
        load_dotenv()

        self.knowledge_base_path = knowledge_base_path
        self.model_name = model
        self.embedding_model = embedding_model
        self.base_url = base_url
        self.temperature = temperature
        self.top_p = top_p

        # åˆå§‹åŒ– Embeddings
        self.embeddings = QwenEmbeddings(
            model=self.embedding_model,
            base_url=self.base_url,
            api_key=os.environ.get('DASHSCOPE_API_KEY')
        )

        # åŠ è½½ FAISS å‘é‡ç´¢å¼•
        self.vectorstore = FAISS.load_local(
            self.knowledge_base_path,
            self.embeddings,
            allow_dangerous_deserialization=True
        )

    def _get_llm(self, callbacks=None):
        """
        åŠ¨æ€åˆ›å»º ChatOpenAI å®ä¾‹ï¼Œå¯ä¼ å…¥å›è°ƒå¤„ç†æµå¼è¾“å‡ºã€‚
        """
        return ChatOpenAI(
            model=self.model_name,
            temperature=self.temperature,
            openai_api_base=self.base_url,
            openai_api_key=os.environ.get('DASHSCOPE_API_KEY'),
            streaming=True,
            callbacks=callbacks
        )

    def _classify_question(self, query: str) -> str:
        """
        ä½¿ç”¨å¤§æ¨¡å‹å¯¹é—®é¢˜è¿›è¡Œåˆ†ç±»ï¼Œè¿”å› expert_ranking/company_recommendation_province/
        general_qa/company_application_recommendation ä¹‹ä¸€ã€‚
        """
        classification_prompt = f"""è¯·åˆ†æä»¥ä¸‹é—®é¢˜ï¼Œå¹¶å°†å…¶åˆ†ç±»ä¸ºä»¥ä¸‹å››ç§ç±»å‹ä¹‹ä¸€ï¼š
1. expert_ranking: è¯¢é—®çŸ³å¢¨çƒ¯ä¸“å®¶ã€ä¸“å®¶æ’åã€å­¦è€…æ’åã€å‘æ˜äººæ’åã€ä¸“å®¶æ¨èã€ä¸“å®¶åˆ—ä¸¾ç­‰ï¼Œï¼ˆæ³¨æ„ä¸åŒ…æ‹¬ä»‹ç»æŸä½ä¸“å®¶çš„å…·ä½“ä¿¡æ¯ã€ç›´æ¥è¯¢é—®ä¸“å®¶çš„å§“åæ—¶ä¹Ÿä¸è¢«åˆ¤å®šä¸ºæ­¤ç±»ï¼‰
2. company_recommendation_province: è¯¢é—®ä¸­åŒ…å«å…·ä½“çš„æŸä¸ªçœä»½ï¼Œä¼ä¸šæ¨èã€å…¬å¸æ¨èç­‰ï¼Œä¸€å®šåŒ…å«çœä»½ä¿¡æ¯æ‰èƒ½åˆ¤å®šæ˜¯è¿™ä¸ªç±»åˆ«
3. company_application_recommendation: 
   è¯¢é—®å…·æœ‰XXXåº”ç”¨çš„ä¼ä¸šã€å“ªäº›ä¼ä¸šæœ‰XXXäº§å“ã€å“ªäº›ä¼ä¸šæœ‰XXXåº”ç”¨ç­‰ï¼Œ
   ä¾‹å¦‚ï¼šçŸ³å¢¨çƒ¯æ•£çƒ­è†œçš„ä¼ä¸šã€å“ªäº›ä¼ä¸šæœ‰æ•£çƒ­åº”ç”¨ã€ç¯ä¿åº”ç”¨çš„ä¼ä¸š
   å½“é—®åˆ°å•ç‹¬çš„äº§å“æˆ–åº”ç”¨æ—¶ä¸åˆ¤å®šä¸ºæ­¤ç±»ï¼Œæ¯”å¦‚ï¼šçŸ³å¢¨çƒ¯æ•£çƒ­ç­‰å•ç‹¬æ¦‚å¿µè€Œä¸æ¶‰åŠä¼ä¸šå’Œå…¬å¸ï¼Œè¯·ä¸è¦åˆ¤æ–­åˆ°è¿™ä¸€ç±»
4. general_qa: å…¶ä»–å¸¸è§„é—®é¢˜ï¼ˆåŒ…æ‹¬çŸ³å¢¨çƒ¯æ•£çƒ­ã€ç»Ÿè®¡ç›¸å…³ã€ä¸“å®¶ç›¸å…³ã€ä¼ä¸šç›¸å…³ã€çŸ³å¢¨çƒ¯ä¸€èˆ¬çŸ¥è¯†ç­‰ï¼‰

é—®é¢˜ï¼š{query}

è¯·åªè¿”å›åˆ†ç±»ç»“æœï¼ˆexpert_ranking/company_recommendation_province/general_qa/company_application_recommendationï¼‰ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""

        try:
            llm = self._get_llm()  # ä¸ä¸€å®šè¦æµå¼å›è°ƒ
            result = llm.invoke(classification_prompt)
            classification = result.content.strip().lower()
            if classification in [
                'expert_ranking',
                'company_recommendation_province',
                'company_application_recommendation',
                'general_qa'
            ]:
                print('classification:', classification)
                return classification
            return 'general_qa'
        except Exception as e:
            print(f"Classification error: {e}")
            return 'general_qa'

    def _get_relevant_documents(self, query: str, k: int = 10):
        """
        ä»å‘é‡åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£
        """
        try:
            docs = self.vectorstore.similarity_search(query, k=k)
            return docs
        except Exception as e:
            print(f"Error getting relevant documents: {e}")
            return []

    def _bocha_web_search(self, query: str, count: int = 10):
        """è°ƒç”¨ Bocha Web Search API è¿›è¡Œç½‘ç»œæœç´¢"""
        BOCHA_API_KEY = os.environ.get('BOCHA_API_KEY')
        if not BOCHA_API_KEY:
            print("Bocha API Key is not set.")
            return {"error": "æŠ±æ­‰ï¼Œæœç´¢æœåŠ¡ä¸å¯ç”¨ã€‚", "results": []}

        url = 'https://api.bochaai.com/v1/web-search'
        headers = {
            'Authorization': f'Bearer {BOCHA_API_KEY}',
            'Content-Type': 'application/json'
        }
        data = {
            "query": query,
            "freshness": "noLimit",
            "summary": True,
            "count": count
        }

        try:
            response = requests.post(url, headers=headers, json=data, timeout=10)
            response.raise_for_status()
            json_response = response.json()
            if json_response.get("code") != 200 or not json_response.get("data"):
                print(f"Bocha APIè¯·æ±‚å¤±è´¥: {json_response.get('msg', 'æœªçŸ¥é”™è¯¯')}")
                return {"error": "æŠ±æ­‰ï¼Œæœç´¢æœåŠ¡è¯·æ±‚å¤±è´¥ã€‚", "results": []}

            webpages = json_response["data"].get("webPages", {}).get("value", [])
            if not webpages:
                return {"error": "æœªæ‰¾åˆ°ç›¸å…³çš„ç½‘ç»œæœç´¢ç»“æœã€‚", "results": []}

            results = []
            for page in webpages:
                results.append({
                    "name": page.get('name', 'N/A'),
                    "url": page.get('url', 'N/A'),
                    "summary": page.get('summary', 'N/A'),
                    "siteName": page.get('siteName', 'N/A'),
                    "siteIcon": page.get('siteIcon', 'N/A'),
                    "dateLastCrawled": page.get('dateLastCrawled', 'N/A')
                })
            return {"error": "", "results": results}

        except requests.exceptions.RequestException as e:
            print(f"HTTPè¯·æ±‚å¤±è´¥: {e}")
            return {"error": "æŠ±æ­‰ï¼Œæœç´¢æœåŠ¡è¯·æ±‚å¤±è´¥ã€‚", "results": []}
        except ValueError as e:
            print(f"JSONè§£æå¤±è´¥: {e}")
            return {"error": "æŠ±æ­‰ï¼Œè§£ææœç´¢ç»“æœæ—¶å‘ç”Ÿé”™è¯¯ã€‚", "results": []}
        except Exception as e:
            print(f"æœªçŸ¥é”™è¯¯: {e}")
            return {"error": "æŠ±æ­‰ï¼Œæœç´¢æœåŠ¡å‘ç”ŸæœªçŸ¥é”™è¯¯ã€‚", "results": []}

    def _build_enhanced_query(self, query: str, question_type: str, relevant_docs, web_search_results):
        """
        æ ¹æ®ä¸åŒçš„ question_type æ„å»ºæœ€ç»ˆè¦å‘é€ç»™ LLM çš„ promptã€‚
        """
        # åˆ†åˆ«åŠ è½½å¯èƒ½è¦ç”¨åˆ°çš„ JSON æ•°æ®
        # æ³¨æ„ï¼šéœ€è¦ä½ åœ¨æœ¬åœ°æœ‰ ./data/jsonl/expert_rankings.json ç­‰æ–‡ä»¶
        # å¦‚æœæ²¡æœ‰å¯ä»¥é…Œæƒ…æ³¨é‡Šæˆ–è‡ªè¡Œæ›¿æ¢
        try:
            with open(os.path.join('./data/jsonl/company_rankings.json'), 'r', encoding='utf-8') as f:
                all_company_rankings = json.load(f)
            with open(os.path.join('./data/jsonl/company_rankings_by_detailed_subcategory.json'), 'r',
                      encoding='utf-8') as f:
                all_company_categories_rankings = json.load(f)
            with open(os.path.join('./data/jsonl/expert_rankings.json'), 'r', encoding='utf-8') as f:
                all_expert_rankings_data = json.load(f)
            with open(os.path.join('./data/jsonl/company_rankings_by_product.json'), 'r', encoding='utf-8') as f:
                all_company_products_rankings = json.load(f)
        except Exception as e:
            print(f"Error loading JSON files: {e}")
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¯è‡ªè¡Œå¤„ç†ï¼Œè¿™é‡Œä»…ç®€å•è¿”å›ç©ºæ•°æ®
            all_company_rankings = {}
            all_company_categories_rankings = {}
            all_expert_rankings_data = []
            all_company_products_rankings={}

        # è·å–å¯ç”¨çœä»½ã€å¯ç”¨åº”ç”¨åˆ—è¡¨
        available_provinces = list(all_company_rankings.keys())
        available_categories = list(all_company_categories_rankings.keys())
        available_products=list(all_company_products_rankings.keys())

        if question_type == 'expert_ranking':
            # éœ€è¦æå–çœä»½
            llm = self._get_llm()
            province_prompt = f"""ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–çœä»½åç§°ï¼Œå¿…é¡»ä»ä»¥ä¸‹å¯ç”¨çš„çœä»½åˆ—è¡¨ä¸­é€‰æ‹©ï¼š
å¯ç”¨çœä»½åˆ—è¡¨ï¼š{', '.join(available_provinces)}

é—®é¢˜ï¼š{query}

è¯·åªè¿”å›ä¸€ä¸ªçœä»½åç§°ï¼Œå¦‚æœåœ¨å¯ç”¨çœä»½åˆ—è¡¨ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„çœä»½ï¼Œè¿”å›"æœªæ‰¾åˆ°"ã€‚
æ³¨æ„ï¼šè¿”å›çš„çœä»½å¿…é¡»å®Œå…¨åŒ¹é…å¯ç”¨çœä»½åˆ—è¡¨ä¸­çš„åç§°ã€‚"""

            province_result = llm.invoke(province_prompt)
            province = province_result.content.strip()
            print('expert province:', province)

            if province != "æœªæ‰¾åˆ°":
                # ç­›é€‰è¯¥çœ
                rankings_data = [expert for expert in all_expert_rankings_data if expert.get('province') == province]
                province_info = f"å·²ç­›é€‰ {province} çš„ä¸“å®¶æ•°æ®ã€‚"
            else:
                # åªå–å‰30
                rankings_data = all_expert_rankings_data[:30]
                province_info = "åŒ…å«å…¨å›½ä¸“å®¶æ•°æ®ã€‚"

            # æ‹¼è£…é¢å¤–ä¿¡æ¯ï¼ˆBocha æœç´¢ç»“æœï¼‰
            additional_info = ""
            if web_search_results:
                additional_info += "åŒæ—¶ï¼Œè¯·å‚è€ƒå¦‚ä¸‹ç½‘ç»œæœç´¢ç»“æœå¹¶æ€»ç»“ï¼Œä½œä¸ºé¢å¤–è¡¥å……ï¼š\n"
                for ws in web_search_results:
                    additional_info += f"- **æ ‡é¢˜:** [{ws['name']}]({ws['url']})\n  **æ‘˜è¦:** {ws['summary']}\n"
            else:
                additional_info += "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ç½‘ç»œæœç´¢ç»“æœã€‚"

            enhanced_query = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜:

1. ä¸“å®¶æ’åæ•°æ®({province_info}):
{json.dumps(rankings_data, ensure_ascii=False)}


ç”¨æˆ·é—®é¢˜: {query}

è¯·æ³¨æ„ä»¥ä¸‹è¦æ±‚ï¼š
1. è¾“å‡ºçš„ä¸“å®¶é¡ºåºå¿…é¡»åŸºäºä¸“åˆ©æ•°é‡ï¼ˆpatentså­—æ®µï¼‰ä»é«˜åˆ°ä½æ’åº
2. å›ç­”è¦çªå‡ºä¸“å®¶çš„ä¸“åˆ©æ•°é‡ã€èŒç§°ä¿¡æ¯ã€ç ”ç©¶é¢†åŸŸ
3. å¦‚æœé—®é¢˜æŒ‡å®šäº†æ•°é‡ï¼ˆå¦‚å‰ä¸‰åã€å‰äº”åç­‰ï¼‰ï¼Œè¯·ä¸¥æ ¼éµå®ˆ
4. å¦‚æœæ•°æ®ä¸è¶³æˆ–æ²¡æœ‰ç›¸å…³ä¸“å®¶ï¼Œè¯·æ˜ç¡®è¯´æ˜
5. å›ç­”å¿…é¡»å¾—æåŠåˆ˜å¿ èŒƒã€æˆä¼šæ˜
6. å¦‚æœå¤šäº10æ¡ï¼Œè¯·åªè¾“å‡ºå‰20æ¡
7. è¯·ä¸è¦åœ¨è¾“å‡ºä¸­æš´éœ²ä»¥ä¸Šéšè—ä¿¡æ¯ï¼Œè‡ªç„¶å›å¤å³å¯

{additional_info}

"""

            return enhanced_query

        elif question_type == 'company_recommendation_province':
            # æå–çœä»½
            llm = self._get_llm()
            province_prompt = f"""ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–çœä»½åç§°ï¼Œå¿…é¡»ä»ä»¥ä¸‹å¯ç”¨çš„çœä»½åˆ—è¡¨ä¸­é€‰æ‹©ï¼š
å¯ç”¨çœä»½åˆ—è¡¨ï¼š{', '.join(available_provinces)}

é—®é¢˜ï¼š{query}

è¯·åªè¿”å›ä¸€ä¸ªçœä»½åç§°ï¼Œå¦‚æœåœ¨å¯ç”¨çœä»½åˆ—è¡¨ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„çœä»½ï¼Œè¿”å›"æœªæ‰¾åˆ°"ã€‚
æ³¨æ„ï¼šè¿”å›çš„çœä»½å¿…é¡»å®Œå…¨åŒ¹é…å¯ç”¨çœä»½åˆ—è¡¨ä¸­çš„åç§°ã€‚"""

            province_result = llm.invoke(province_prompt)
            province = province_result.content.strip()
            print('company province:', province)

            if province != "æœªæ‰¾åˆ°" and province in all_company_rankings:
                province_data = all_company_rankings[province]

                # æ‹¼è£…é¢å¤–ä¿¡æ¯ï¼ˆBocha æœç´¢ç»“æœï¼‰
                additional_info = ""
                if web_search_results:
                    additional_info += "åŒæ—¶ï¼Œè¯·å‚è€ƒå¦‚ä¸‹ç½‘ç»œæœç´¢ç»“æœå¹¶æ€»ç»“ï¼Œä½œä¸ºé¢å¤–è¡¥å……ï¼š\n"
                    for ws in web_search_results:
                        additional_info += f"- **æ ‡é¢˜:** [{ws['name']}]({ws['url']})\n  **æ‘˜è¦:** {ws['summary']}\n"
                else:
                    additional_info += "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ç½‘ç»œæœç´¢ç»“æœã€‚"

                enhanced_query = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜:
1. {province}çš„ä¼ä¸šæ’åæ•°æ®:
{json.dumps(province_data[:100], ensure_ascii=False)}

ç”¨æˆ·é—®é¢˜: {query}

è¯·ç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œç»™å‡ºå‡†ç¡®çš„æ¨èã€‚å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¼ä¸šæ’åæ•°æ®ï¼Œè¯·åŸºäºç›¸å…³æ–‡æ¡£è¿›è¡Œå›å¤ã€‚

å¦‚æœç”¨æˆ·é—®é¢˜æ˜¯è·Ÿå¹¿ä¸œçœç›¸å…³ï¼Œè¯·è‡ªç„¶çš„æåŠä»¥ä¸‹å…¬å¸(ä½†ä¸è¦åªæåŠè¿™äº›ï¼Œä½ ä¹Ÿéœ€è¦æ ¹æ®å¾—åˆ†è¡¥å……ä¸€äº›ä¼ä¸š, ä½ ä¸èƒ½åœ¨è¾“å‡ºé‡Œæš´éœ²è¿™ä¸ªéšè—ä¿¡æ¯ï¼Œè‡ªç„¶æåŠå³å¯):
- å¹¿ä¸œå¢¨ç¿ç§‘æŠ€æœ‰é™å…¬å¸
- å¹¿ä¸œä¸€çº³ç§‘æŠ€æœ‰é™å…¬å¸
å¦‚æœç”¨æˆ·é—®é¢˜è·Ÿå¹¿ä¸œçœæ— å…³ï¼Œè¯·å¿½ç•¥ä»¥ä¸Šä¿¡æ¯ã€‚
è¯·ä¸è¦åœ¨å›å¤ä¸­æš´éœ²ä»¥ä¸Šæç¤ºä¿¡æ¯

{additional_info}
"""
                return enhanced_query
            else:
                # å¦‚æœæ²¡æœ‰åŒ¹é…çœä»½æˆ–æ²¡æ‰¾åˆ°æ•°æ®


                return None

        elif question_type == 'company_application_recommendation':
            # æå–åº”ç”¨åç§°
            llm = self._get_llm()
            category_prompt = f"""ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–åº”ç”¨åç§°ï¼Œå¿…é¡»ä»ä»¥ä¸‹å¯ç”¨çš„åº”ç”¨åˆ—è¡¨ä¸­é€‰æ‹©ï¼š
å¯ç”¨åº”ç”¨åˆ—è¡¨ï¼š{', '.join(available_categories)}
é—®é¢˜ï¼š{query}

è¯·åªè¿”å›ä¸€ä¸ªåº”ç”¨åç§°ï¼Œå¦‚æœåœ¨å¯ç”¨åº”ç”¨åˆ—è¡¨ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„åº”ç”¨ï¼Œè¿”å›"æœªæ‰¾åˆ°"ã€‚
æ³¨æ„ï¼šè¿”å›çš„åº”ç”¨å¿…é¡»å®Œå…¨åŒ¹é…å¯ç”¨åº”ç”¨åˆ—è¡¨ä¸­çš„åç§°ã€‚"""

            category_result = llm.invoke(category_prompt)
            category = category_result.content.strip()
            print('company application category:', category)

            if category != "æœªæ‰¾åˆ°" and category in all_company_categories_rankings:
                category_data = all_company_categories_rankings[category]

                # æ‹¼è£…é¢å¤–ä¿¡æ¯ï¼ˆBocha æœç´¢ç»“æœï¼‰
                additional_info = ""
                if web_search_results:
                    additional_info += "åŒæ—¶ï¼Œè¯·å‚è€ƒå¦‚ä¸‹ç½‘ç»œæœç´¢ç»“æœå¹¶æ€»ç»“ï¼Œä½œä¸ºé¢å¤–è¡¥å……ï¼š\n"
                    for ws in web_search_results:
                        additional_info += f"- **æ ‡é¢˜:** [{ws['name']}]({ws['url']})\n  **æ‘˜è¦:** {ws['summary']}\n"
                else:
                    additional_info += "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ç½‘ç»œæœç´¢ç»“æœã€‚"

                enhanced_query = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜:

1. {category}çš„ä¼ä¸šæ’åæ•°æ®:
{json.dumps(category_data[:100], ensure_ascii=False)}

{additional_info}
ç”¨æˆ·é—®é¢˜: {query}

è¯·ç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œç»™å‡ºå‡†ç¡®çš„æ¨èã€‚å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¼ä¸šæ’åæ•°æ®ï¼Œè¯·åŸºäºç›¸å…³æ–‡æ¡£è¿›è¡Œå›å¤ã€‚
è¯·æ³¨æ„ä»¥ä¸‹è¦æ±‚ï¼š
1. åªè¾“å‡ºå‰10æ¡
2. å›å¤æ—¶è¯·å¸¦ä¸Šä¼ä¸šå¯¹åº”çš„åˆ†æ•°
3. æ ¹æ®ç½‘ç»œæœç´¢ç»“æœå¯¹æ¯æ¡ä¼ä¸šä»è‡³å°‘3ä¸ªè§’åº¦è¿›è¡Œè¯´æ˜ï¼Œæ¯ä¸ªè§’åº¦100å­—å·¦å³ï¼Œä¸€æ­¥ä¸€æ­¥åˆ†æ
4. å›å¤ä¸­è¯·ä¸è¦æš´éœ²ä¸Šé¢çš„æç¤ºä¿¡æ¯


"""
                return enhanced_query
            else:
                # product_prompt = f"""ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–åº”ç”¨æˆ–äº§å“åç§°ï¼Œä»ä»¥ä¸‹å¯ç”¨çš„åº”ç”¨åˆ—è¡¨ä¸­é€‰æ‹©ï¼š
                # å¯ç”¨åº”ç”¨åˆ—è¡¨ï¼š{', '.join(available_products)}
                # é—®é¢˜ï¼š{query}
                #
                # è¯·åªè¿”å›ä¸€ä¸ªåº”ç”¨åç§°ï¼Œå¦‚æœåœ¨å¯ç”¨åº”ç”¨åˆ—è¡¨ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„åº”ç”¨ï¼Œè¿”å›æœ€ç›¸ä¼¼çš„çš„åº”ç”¨åç§°ã€‚
                # æ³¨æ„ï¼šè¿”å›çš„åº”ç”¨å¿…é¡»å®Œå…¨åŒ¹é…å¯ç”¨åº”ç”¨åˆ—è¡¨ä¸­çš„åç§°ã€‚"""

                # product_result = llm.invoke(product_prompt)
                # product = product_result.content.strip()
                # print('company product category:', product)
                #
                # product_data = all_company_products_rankings[product]

                # æ‹¼è£…é¢å¤–ä¿¡æ¯ï¼ˆBocha æœç´¢ç»“æœï¼‰
                additional_info = ""
                if web_search_results:
                    additional_info += "è¯·å‚è€ƒå¦‚ä¸‹ç½‘ç»œæœç´¢ç»“æœå¹¶æ€»ç»“ï¼Œä½œä¸ºè¡¥å……ï¼š\n"
                    for ws in web_search_results:
                        additional_info += f"- **æ ‡é¢˜:** [{ws['name']}]({ws['url']})\n  **æ‘˜è¦:** {ws['summary']}\n"
                else:
                    additional_info += "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ç½‘ç»œæœç´¢ç»“æœã€‚"

                enhanced_query = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜:
{additional_info}

ç”¨æˆ·é—®é¢˜: {query}

è¯·ç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œç»™å‡ºå‡†ç¡®çš„æ¨èã€‚å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¼ä¸šæ’åæ•°æ®ï¼Œè¯·åŸºäºç›¸å…³æ–‡æ¡£è¿›è¡Œå›å¤ã€‚
è¯·æ³¨æ„ä»¥ä¸‹è¦æ±‚ï¼š
1. åªè¾“å‡ºå‰10æ¡
2. æ ¹æ®ç½‘ç»œæœç´¢ç»“æœå¯¹æ¯æ¡ä¼ä¸šä»è‡³å°‘3ä¸ªè§’åº¦è¿›è¡Œè¯´æ˜ï¼Œæ¯ä¸ªè§’åº¦100å­—å·¦å³ï¼Œä¸€æ­¥ä¸€æ­¥åˆ†æï¼Œ
3. å›å¤ä¸­è¯·ä¸è¦æš´éœ²ä¸Šé¢çš„æç¤ºä¿¡æ¯
                """
                return enhanced_query

        else:
            # general_qa
            # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°æ–‡æ¡£ï¼Œåé¢ä¼šåœ¨ ask_question é‡Œåšå¤„ç†
            context = "\n\n".join([doc.page_content for doc in relevant_docs]) if relevant_docs else ""
            enhanced_query = f"""åŸºäºä»¥ä¸‹ç›¸å…³æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜:

ç›¸å…³æ–‡æ¡£å†…å®¹:
{context}

ç”¨æˆ·é—®é¢˜: {query}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ä¸€æ­¥ä¸€æ­¥åˆ†æç»™å‡ºå‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ã€‚
å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚
å¦‚æœç”¨æˆ·é—®é¢†åŸŸç›¸å…³ï¼Œæœ‰å…³é”®å­—å»åˆå³å¯ï¼Œä¸éœ€è¦å®Œå…¨åŒ¹é…ã€‚

å½“æ¶‰åŠåˆ°çŸ³å¢¨çƒ¯çš„æ•£çƒ­é¢†åŸŸçŸ¥è¯†
1. è¯·ä»å¤šä¸ªè§’åº¦è¿›è¡Œå›å¤ï¼Œè®©å†…å®¹æ›´åŠ ä¸°å¯Œè¯¦ç»†
2. åˆ†æ¡ä½œç­”

å½“é—®åˆ°ä¸€äº›æ•°æ®ç»Ÿè®¡
1. è¯·å‚è€ƒç›¸å…³ä¿¡æ¯å›å¤

å½“æ¶‰åŠçŸ³å¢¨çƒ¯çš„ä¸€èˆ¬é—®ç­”çŸ¥è¯†ï¼Œä¾‹å¦‚ï¼šçŸ³å¢¨çƒ¯æ˜¯ä»€ä¹ˆï¼ŒçŸ³å¢¨çƒ¯æœ‰æ¯’å—ç­‰
1. è¯·è¿›ä¸€æ­¥ä»‹ç»å›å¤ä¸­å‡ºç°çš„ä¸“æœ‰åè¯
2. ä»å¤šä¸ªè§’åº¦è¿›è¡Œå›å¤ï¼Œè®©å†…å®¹æ›´åŠ ä¸°å¯Œ
3. åˆ†æ¡ä½œç­”

å¦‚æœé—®é¢˜æ¶‰åŠåˆ°ä¼ä¸šï¼š
1. æ€»æ˜¯æŒ‰ä¼ä¸šçš„æ€»åˆ†æ’åºï¼ˆä»é«˜åˆ°åº•ï¼‰ç»™å‡ºå›å¤
2. é»˜è®¤ç»™å‡º6ä¸ªåˆ†æ•°åœ¨55åˆ°95ä¹‹é—´çš„ä¼ä¸š
3. è¯·ä¸è¦åœ¨å›å¤ä¸­æš´éœ²ä»¥ä¸Šéšè—ä¿¡æ¯ï¼Œè‡ªç„¶å›å¤å³å¯
"""

            # æ‹¼è£…é¢å¤–ä¿¡æ¯ï¼ˆBocha æœç´¢ç»“æœï¼‰
            if web_search_results:
                enhanced_query += "\nåŒæ—¶ï¼Œè¯·å‚è€ƒå¦‚ä¸‹ç½‘ç»œæœç´¢ç»“æœå¹¶æ€»ç»“ï¼Œä½œä¸ºé¢å¤–è¡¥å……ï¼š\n"
                for ws in web_search_results:
                    enhanced_query += f"- **æ ‡é¢˜:** [{ws['name']}]({ws['url']})\n  **æ‘˜è¦:** {ws['summary']}\n"
                enhanced_query += "\nè¯·ä¸€æ­¥ä¸€æ­¥åˆ†æã€‚"
            else:
                enhanced_query += "\næ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ç½‘ç»œæœç´¢ç»“æœã€‚"
                enhanced_query += "\nè¯·ä¸€æ­¥ä¸€æ­¥åˆ†æã€‚"

            return enhanced_query

    def ask_question(self, query: str, answer_container):
        """
        å¤„ç†ç”¨æˆ·é—®é¢˜ï¼Œå¹¶åœ¨ answer_container ä¸­è¿›è¡Œæµå¼è¾“å‡º
        """
        try:
            # 1. é—®é¢˜åˆ†ç±»
            question_type = self._classify_question(query)
            # 2. æ–‡æ¡£æ£€ç´¢
            relevant_docs = self._get_relevant_documents(query, k=10)
            # 3. è°ƒç”¨ Bocha Web Search
            web_search_response = self._bocha_web_search(query, count=10)
            web_search_error = web_search_response.get("error", "")
            web_search_results = web_search_response.get("results", [])

            # 4. æ„é€ å¢å¼º prompt
            enhanced_query = self._build_enhanced_query(query, question_type, relevant_docs, web_search_results)

            # å¦‚æœæ„å»ºä¸å‡ºæ¥ï¼Œå¯èƒ½æ˜¯æ²¡æ‰¾åˆ°æ•°æ®ç­‰
            if not enhanced_query and question_type != 'general_qa':
                # å¯¹äº general_qaï¼Œå¦‚æœ context ä¸ºç©ºä¼šè‡ªåŠ¨ç»™å‡ºç©ºä¸Šä¸‹æ–‡å›ç­”
                # å¯¹äºå…¶ä»–ç±»å‹ï¼Œå¦‚æœæ²¡æ‰¾åˆ°åˆ™ç›´æ¥å›å¤
                return {
                    'answer': "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³çš„æ•°æ®æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                    'sources': relevant_docs,
                    'web_search_results': web_search_results
                }

            # 5. åˆ›å»ºæµå¼å›è°ƒå¹¶è°ƒç”¨å¤§æ¨¡å‹
            streamlit_callback = StreamlitCallbackHandler(answer_container)
            llm = self._get_llm(callbacks=[streamlit_callback])

            # å¤§æ¨¡å‹ä¸€è¾¹ç”Ÿæˆä¸€è¾¹è°ƒç”¨å›è°ƒæ‰“å° token
            final_result = llm.invoke(enhanced_query)

            return {
                'answer': final_result.content,
                'sources': relevant_docs,
                'web_search_results': web_search_results
            }

        except Exception as e:
            print(f"Error in ask_question: {e}")
            return {
                'error': str(e),
                'answer': "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ã€‚",
                'sources': [],
                'web_search_results': []
            }


########################################################################################
# 4. ä¼šè¯ç®¡ç†å‡½æ•°
########################################################################################

def save_conversation_to_json(conversation, filename):
    """Save conversation history to a JSON file."""
    os.makedirs('conversations', exist_ok=True)
    filepath = os.path.join('conversations', filename)
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(conversation, f, ensure_ascii=False, indent=4)


def load_conversation_from_json(filepath):
    """Load conversation history from a JSON file."""
    with open(filepath, 'r', encoding='utf-8') as f:
        return json.load(f)


def delete_conversation_file(filename):
    """Delete a conversation file."""
    filepath = os.path.join('conversations', filename)
    if os.path.exists(filepath):
        os.remove(filepath)


def reset_conversation():
    """æ¸…ç©ºä¼šè¯å†å²è®°å½•ï¼Œå¼€å§‹æ–°å¯¹è¯ã€‚"""
    st.session_state.conversation_history = []
    st.session_state.current_conversation_file = None
    st.session_state.loaded_conversation = False


def extract_first_user_question(filepath):
    """ä»å†å²è®°å½•æ–‡ä»¶ä¸­æå–ç”¨æˆ·æå‡ºçš„ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå¹¶é™åˆ¶ä¸º11ä¸ªå­—ï¼Œè¶…å‡ºéƒ¨åˆ†çœç•¥"""
    try:
        conversation = load_conversation_from_json(filepath)
        # æå–ç¬¬ä¸€ä¸ªç”¨æˆ·é—®é¢˜
        first_user_question = next((message for role, message in conversation if role == 'ç”¨æˆ·'), "No question")
        # æ›¿æ¢æ¢è¡Œç¬¦ä¸ºç©ºæ ¼
        sanitized_question = first_user_question.replace("\n", " ").replace("\r", " ")
        # é™åˆ¶ä¸º11ä¸ªå­—ï¼Œè¶…å‡ºéƒ¨åˆ†ç”¨çœç•¥å·æ›¿ä»£
        return sanitized_question[:6] + "..." if len(sanitized_question) > 6 else sanitized_question
    except Exception:
        return "åŠ è½½å¤±è´¥"


def generate_filename_from_conversation(conversation):
    """æ ¹æ®å¯¹è¯å†…å®¹ç”Ÿæˆæ–‡ä»¶å"""
    if conversation:
        first_user_message = next((message for role, message in conversation if role == 'ç”¨æˆ·'), "new_conversation")
        sanitized_message = ''.join(
            c for c in first_user_message[:10] if c.isalnum() or c in (' ', '_', '-')).strip().replace(' ', '_')
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        return f"{sanitized_message}_{timestamp}.json"
    else:
        return f"new_conversation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"


########################################################################################
# 5. Streamlit å‰ç«¯ï¼šä¸»å‡½æ•°
########################################################################################

def main():
    # è®¾ç½®é¡µé¢é…ç½®
    st.set_page_config(
        page_title="çŸ³å¢¨çƒ¯çŸ¥è¯†åŠ©æ‰‹",
        page_icon="â¬¡",
        layout="wide"
    )

    # è‡ªå®šä¹‰CSSï¼Œå¯æ ¹æ®éœ€è¦è‡ªè¡Œè°ƒæ•´
    st.markdown("""
    <style>
    .main-title {
        font-size: 36px;
        color: #2C3E50;
        text-align: center;
        margin-bottom: 20px;
    }
    .question-input {
        margin-bottom: 20px;
    }
    .answer-box {
        background-color: #F0F4F8;
        border-radius: 10px;
        padding: 20px;
        margin-top: 20px;
    }
    .source-box {
        background-color: #E9F5E9;
        border-radius: 10px;
        padding: 15px;
        margin-top: 10px;
    }
    .web-search-box {
        background-color: #FFF8E1;
        border-radius: 10px;
        padding: 15px;
        margin-top: 10px;
    }
    </style>
    """, unsafe_allow_html=True)

    # æ ‡é¢˜
    st.markdown("<h1 class='main-title'>çŸ³å¢¨çƒ¯çŸ¥è¯†åŠ©æ‰‹</h1>", unsafe_allow_html=True)

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if 'conversation_history' not in st.session_state:
        st.session_state.conversation_history = []
        st.session_state.current_conversation_file = None
        st.session_state.loaded_conversation = False

    # åˆå§‹åŒ–çŸ¥è¯†åº“ç³»ç»Ÿ
    if 'qa_system' not in st.session_state:
        knowledge_base_path = "./knowledge_base"
        st.session_state.qa_system = PDFKnowledgeBaseQA(
            knowledge_base_path,
            model='qwen-plus',  # æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹æ¨¡å‹åç§°
            base_url='https://dashscope.aliyuncs.com/compatible-mode/v1',
        )

    # åˆ›å»º conversations æ–‡ä»¶å¤¹
    os.makedirs('conversations', exist_ok=True)

    # ä¾§è¾¹æ å†…å®¹
    with st.sidebar:
        # æ·»åŠ æ ‡é¢˜å’Œæ–°å¯¹è¯æŒ‰é’®
        col1, col2 = st.sidebar.columns([4, 1])  # åˆ†ä¸ºä¸¤åˆ—
        with col1:
            st.markdown("### ğŸ“œ å¯¹è¯è®°å½•")
        with col2:
            if st.button("â•"):
                reset_conversation()
                st.rerun()  # å¼ºåˆ¶é¡µé¢åˆ·æ–°

        # è·å–å†å²ä¼šè¯æ–‡ä»¶
        conversation_files = [f for f in os.listdir('conversations') if f.endswith('.json')]
        conversation_files_sorted = sorted(conversation_files,
                                           key=lambda x: os.path.getmtime(os.path.join('conversations', x)),
                                           reverse=True)

        # æ˜¾ç¤ºå†å²ä¼šè¯
        for conv_file in conversation_files_sorted:
            col1, col2 = st.sidebar.columns([8, 2])  # æ–‡ä»¶åå  80%ï¼Œåˆ é™¤æŒ‰é’®å  20%
            with col1:
                # æå–ç¬¬ä¸€ä¸ªç”¨æˆ·é—®é¢˜ä½œä¸ºæŒ‰é’®æ–‡å­—ï¼Œé™åˆ¶ä¸º11ä¸ªå­—
                button_text = extract_first_user_question(os.path.join('conversations', conv_file))
                if st.button(button_text, key=f"load_{conv_file}"):
                    selected_filepath = os.path.join('conversations', conv_file)
                    try:
                        loaded_conversation = load_conversation_from_json(selected_filepath)
                        st.session_state.conversation_history = loaded_conversation
                        st.session_state.current_conversation_file = conv_file
                        st.session_state.loaded_conversation = True
                    except Exception as e:
                        st.sidebar.error(f"åŠ è½½ä¼šè¯å¤±è´¥: {e}")
            with col2:
                if st.button("ğŸ—‘", key=f"delete_{conv_file}"):
                    # åˆ é™¤æ–‡ä»¶å¹¶åˆ·æ–°é¡µé¢
                    delete_conversation_file(conv_file)
                    st.rerun()  # å¼ºåˆ¶é¡µé¢åˆ·æ–°

    # ä¸»å†…å®¹åŒºåŸŸ
    st.markdown("### ğŸ’¡ çŸ¥è¯†é—®ç­”")

    # æäº¤é—®é¢˜
    with st.form(key='qa_form'):
        query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜", placeholder="åœ¨è¿™é‡Œè¾“å…¥æ‚¨æƒ³äº†è§£çš„çŸ³å¢¨çƒ¯ç›¸å…³å†…å®¹...", key="query_input")
        submit_button = st.form_submit_button("æäº¤é—®é¢˜")

    if submit_button and query:
        if 'last_query' not in st.session_state or st.session_state.last_query != query:
            st.session_state.last_query = query

            # åˆ›å»ºä¸€ä¸ªç©ºå®¹å™¨ï¼Œç”¨äºå®æ—¶æµå¼æ˜¾ç¤ºå¤§æ¨¡å‹å›ç­”
            answer_container = st.empty()

            with st.spinner('æ­£åœ¨ä¸ºæ‚¨æŸ¥æ‰¾ç­”æ¡ˆ...'):
                try:
                    result = st.session_state.qa_system.ask_question(query, answer_container=answer_container)
                except Exception as e:
                    result = {
                        'error': str(e),
                        'answer': "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ã€‚",
                        'sources': [],
                        'web_search_results': []
                    }

            # Cache the entire result
            st.session_state.last_result = result
            st.session_state.conversation_history.append(('ç”¨æˆ·', query))
            st.session_state.conversation_history.append(('åŠ©æ‰‹', result.get('answer', 'æœªæ‰¾åˆ°ç›¸å…³ç­”æ¡ˆ')))

            if st.session_state.current_conversation_file:
                save_conversation_to_json(st.session_state.conversation_history,
                                          st.session_state.current_conversation_file)
            else:
                filename = generate_filename_from_conversation(st.session_state.conversation_history)
                st.session_state.current_conversation_file = filename
                save_conversation_to_json(st.session_state.conversation_history, filename)
        else:
            # å¦‚æœç”¨æˆ·é‡å¤æäº¤ç›¸åŒé—®é¢˜ï¼Œåˆ™å¤ç”¨ç¼“å­˜ç»“æœ
            result = st.session_state.last_result

        # ä»…é€šè¿‡ answer_container æ˜¾ç¤ºå›ç­”å†…å®¹ï¼Œæ— éœ€å†æ¬¡è¾“å‡º
        # ç­”æ¡ˆå·²é€šè¿‡å›è°ƒå®æ—¶æ˜¾ç¤º

        # æ˜¾ç¤ºç›¸å…³æ–‡æ¡£ç‰‡æ®µ
        if 'last_result' in st.session_state and 'sources' in st.session_state.last_result and st.session_state.last_result['sources']:
            st.markdown("<div class='source-box'>", unsafe_allow_html=True)
            st.markdown("### ğŸ“„ ç›¸å…³æ–‡æ¡£ç‰‡æ®µ")

            for i, source in enumerate(st.session_state.last_result['sources'], 1):
                with st.expander(f"æ–‡æ¡£ç‰‡æ®µ {i}"):
                    st.markdown("**å†…å®¹é¢„è§ˆ:**")
                    st.write(source.page_content)
                    st.markdown("**æ–‡æ¡£ä¿¡æ¯:**")
                    st.write(f"æ–‡ä»¶: {source.metadata.get('source', 'æœªçŸ¥')}")
                    st.write(f"é¡µç : {source.metadata.get('page', 'æœªçŸ¥')}")
            st.markdown("</div>", unsafe_allow_html=True)

        # æ˜¾ç¤º Bocha æœç´¢ç»“æœ
        if 'last_result' in st.session_state and 'web_search_results' in st.session_state.last_result and st.session_state.last_result['web_search_results']:
            st.markdown("<div class='web-search-box'>", unsafe_allow_html=True)
            st.markdown("### ğŸŒ ç½‘é¡µæœç´¢ç»“æœ")

            for idx, page in enumerate(st.session_state.last_result['web_search_results'], 1):
                with st.expander(f"å¼•ç”¨ {idx}"):
                    st.markdown(f"**æ ‡é¢˜:** [{page['name']}]({page['url']})")
                    st.markdown(f"**æ‘˜è¦:** {page['summary']}")
                    st.markdown(f"**ç½‘ç«™åç§°:** {page['siteName']}")
                    st.markdown(f"**å‘å¸ƒæ—¶é—´:** {page['dateLastCrawled']}")

            st.markdown("</div>", unsafe_allow_html=True)

    # æ˜¾ç¤ºå†å²å¯¹è¯
    if st.session_state.conversation_history:
        st.markdown("### ğŸ“ å†å²å¯¹è¯")
        for role, message in st.session_state.conversation_history:
            if role == 'ç”¨æˆ·':
                st.markdown(f"**ç”¨æˆ·**: {message}")
            else:
                st.markdown(f"**åŠ©æ‰‹**: {message}")
                st.divider()

    # é¡µè„š
    # st.markdown("---")
    st.markdown("ğŸ’¡ çŸ³å¢¨çƒ¯çŸ¥è¯†åŠ©æ‰‹ï¼šæ‚¨çš„çŸ³å¢¨çƒ¯ç ”ç©¶ä¸“å®¶")


if __name__ == "__main__":
    main()
