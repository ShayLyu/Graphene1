import os
import streamlit as st
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv
import json
import re
from openai import OpenAI
from langchain.embeddings.base import Embeddings
from typing import List

class QwenEmbeddings(Embeddings):
    def __init__(
        self,
        api_key: str = None,
        model: str = "text-embedding-v3",
        base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1",
        dimensions: int = 1024
    ):
        self.client = OpenAI(
            api_key=api_key or os.environ.get('DASHSCOPE_API_KEY'),
            base_url=base_url
        )
        self.model = model
        self.dimensions = dimensions

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """å°†æ–‡æ¡£è½¬æ¢ä¸ºå‘é‡"""
        try:
            # ç¡®ä¿æ‰€æœ‰æ–‡æœ¬éƒ½æ˜¯å­—ç¬¦ä¸²
            texts = [str(text) for text in texts]
            response = self.client.embeddings.create(
                model=self.model,
                input=texts,
                dimensions=self.dimensions,
                encoding_format="float"
            )
            return [item.embedding for item in response.data]
        except Exception as e:
            print(f"Embedding error in embed_documents: {e}")
            raise

    def embed_query(self, text: str) -> List[float]:
        """å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡"""
        try:
            text = str(text)  # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
            response = self.client.embeddings.create(
                model=self.model,
                input=[text],
                dimensions=self.dimensions,
                encoding_format="float"
            )
            return response.data[0].embedding
        except Exception as e:
            print(f"Embedding error in embed_query: {e}")
            raise

class PDFKnowledgeBaseQA:
    def __init__(
        self, 
        knowledge_base_path: str,
        model: str = 'qwen-max',
        embedding_model: str = 'text-embedding-v3',
        base_url: str = 'https://dashscope.aliyuncs.com/compatible-mode/v1',
        temperature: float = 0.7
    ):
        load_dotenv()
        
        self.knowledge_base_path = knowledge_base_path
        self.embeddings = QwenEmbeddings(
            model=embedding_model,
            base_url=base_url,
            api_key=os.environ.get('DASHSCOPE_API_KEY')
        )
        
        self.vectorstore = FAISS.load_local(
            knowledge_base_path, 
            self.embeddings,
            allow_dangerous_deserialization=True
        )
        
        self.llm = ChatOpenAI(
            model=model,
            temperature=temperature,
            openai_api_base=base_url,
            openai_api_key=os.environ.get('DASHSCOPE_API_KEY')
        )
    
    def _classify_question(self, query: str) -> str:
        """ä½¿ç”¨å¤§æ¨¡å‹å¯¹é—®é¢˜è¿›è¡Œåˆ†ç±»"""
        classification_prompt = f"""è¯·åˆ†æä»¥ä¸‹é—®é¢˜ï¼Œå¹¶å°†å…¶åˆ†ç±»ä¸ºä»¥ä¸‹ä¸‰ç§ç±»å‹ä¹‹ä¸€ï¼š
        1. expert_ranking: è¯¢é—®ä¸“å®¶æ’åã€å­¦è€…æ’åã€å‘æ˜äººæ’åã€ä¸“å®¶æ¨èã€ä¸“å®¶åˆ—ä¸¾ç­‰
        2. company_recommendation: è¯¢é—®æŸä¸ªçœä»½çš„ä¼ä¸šæ¨èã€å…¬å¸æ¨èç­‰
        3. company_application_recommendation: è¯¢é—®å…·æœ‰XXXåº”ç”¨çš„ä¼ä¸šã€å“ªäº›ä¼ä¸šæœ‰XXXäº§å“ã€å“ªäº›ä¼ä¸šæœ‰XXXåº”ç”¨ç­‰ï¼Œï¼ˆè¯¢é—®ä¸­ä¸€å®šå¸¦æœ‰ä¼ä¸šæˆ–å…¬å¸è¿™ä¸¤ä¸ªå…³é”®å­—ï¼‰
        3. general_qa: å…¶ä»–å¸¸è§„é—®é¢˜ï¼ˆåŒ…æ‹¬è¯¢é—®ä¸“å®¶å…·ä½“ä¿¡æ¯ï¼ŒæŸé¢†åŸŸæœ‰å“ªäº›ä¸“å®¶ï¼ŒXXXä¸“å®¶æœ‰å“ªäº›ä¸“åˆ©)
        
        é—®é¢˜ï¼š{query}
        
        è¯·åªè¿”å›åˆ†ç±»ç»“æœï¼ˆexpert_ranking/company_recommendation/general_qa/company_application_recommendationï¼‰ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ã€‚"""
        
        try:
            result = self.llm.invoke(classification_prompt)
            classification = result.content.strip().lower()
            if classification in ['expert_ranking', 'company_recommendation', 'general_qa','company_application_recommendation']:
                print('classification', classification)
                return classification
            return 'general_qa'
        except Exception as e:
            print(f"Classification error: {e}")
            return 'general_qa'
    
    def _get_relevant_documents(self, query: str, k: int = 10):
        """è·å–ç›¸å…³æ–‡æ¡£"""
        try:
            docs = self.vectorstore.similarity_search(query, k=k)
            return docs
        except Exception as e:
            print(f"Error getting relevant documents: {e}")
            return []
    
    def ask_question(self, query: str):
        """å¤„ç†ç”¨æˆ·é—®é¢˜"""
        try:
            # è·å–å¯ç”¨çš„çœä»½åˆ—è¡¨
            company_rankings_file = os.path.join('./data/jsonl/company_rankings.json')
            company_categoris_ranking_file=os.path.join('./data/jsonl/company_rankings_by_detailed_subcategory.json')
            with open(company_rankings_file, 'r', encoding='utf-8') as f:
                all_company_rankings = json.load(f)
            with open(company_categoris_ranking_file, 'r', encoding='utf-8') as f:
                all_company_categories_rankings = json.load(f)
            available_provinces = list(all_company_rankings.keys())
            available_categories=list(all_company_categories_rankings.keys())
            
            # ä½¿ç”¨å¤§æ¨¡å‹åˆ†ç±»é—®é¢˜
            question_type = self._classify_question(query)
            
            # è·å–ç›¸å…³æ–‡æ¡£
            relevant_docs = self._get_relevant_documents(query)
            
            if question_type == 'expert_ranking':
                # è·å–ä¸“å®¶æ’åæ•°æ®
                rankings_file = os.path.join('./data/jsonl/expert_rankings.json')
                try:
                    with open(rankings_file, 'r', encoding='utf-8') as f:
                        all_rankings_data = json.load(f)
                        
                    # ä½¿ç”¨å¤§æ¨¡å‹æå–çœä»½ï¼Œæä¾›å¯ç”¨çš„çœä»½åˆ—è¡¨
                    province_prompt = f"""ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–çœä»½åç§°ï¼Œå¿…é¡»ä»ä»¥ä¸‹å¯ç”¨çš„çœä»½åˆ—è¡¨ä¸­é€‰æ‹©ï¼š
                    å¯ç”¨çœä»½åˆ—è¡¨ï¼š{', '.join(available_provinces)}
                    
                    é—®é¢˜ï¼š{query}
                    
                    è¯·åªè¿”å›ä¸€ä¸ªçœä»½åç§°ï¼Œå¦‚æœåœ¨å¯ç”¨çœä»½åˆ—è¡¨ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„çœä»½ï¼Œè¿”å›"æœªæ‰¾åˆ°"ã€‚
                    æ³¨æ„ï¼šè¿”å›çš„çœä»½å¿…é¡»å®Œå…¨åŒ¹é…å¯ç”¨çœä»½åˆ—è¡¨ä¸­çš„åç§°ã€‚"""

                    province_result = self.llm.invoke(province_prompt)
                    province = province_result.content.strip()
                    print('expert province:', province)
                    
                    # å¦‚æœæ‰¾åˆ°çœä»½ï¼Œç­›é€‰è¯¥çœçš„ä¸“å®¶
                    if province != "æœªæ‰¾åˆ°":
                        rankings_data = [expert for expert in all_rankings_data if expert.get('province') == province]
                        province_info = f"å·²ç­›é€‰{province}çš„ä¸“å®¶æ•°æ®ã€‚"
                    else:
                        rankings_data = all_rankings_data[:30]
                        province_info = "åŒ…å«å…¨å›½ä¸“å®¶æ•°æ®ã€‚"
                        
                    # æ„å»ºå¢å¼ºçš„é—®é¢˜
                    enhanced_query = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜:

1. ä¸“å®¶æ’åæ•°æ®({province_info}):
{json.dumps(rankings_data, ensure_ascii=False)}


ç”¨æˆ·é—®é¢˜: {query}

è¯·æ³¨æ„ä»¥ä¸‹è¦æ±‚ï¼š
1. è¾“å‡ºçš„ä¸“å®¶é¡ºåºå¿…é¡»åŸºäºä¸“åˆ©æ•°é‡ï¼ˆpatentså­—æ®µï¼‰ä»é«˜åˆ°ä½æ’åº
2. å›ç­”è¦çªå‡ºä¸“å®¶çš„ä¸“åˆ©æ•°é‡ã€èŒç§°ä¿¡æ¯ã€ç ”ç©¶é¢†åŸŸ
3. å¦‚æœé—®é¢˜æŒ‡å®šäº†æ•°é‡ï¼ˆå¦‚å‰ä¸‰åã€å‰äº”åç­‰ï¼‰ï¼Œè¯·ä¸¥æ ¼éµå®ˆ
4. å¦‚æœæ•°æ®ä¸è¶³æˆ–æ²¡æœ‰ç›¸å…³ä¸“å®¶ï¼Œè¯·æ˜ç¡®è¯´æ˜
5. å›ç­”å¿…é¡»å¾—æåŠåˆ˜å¿ èŒƒã€æˆä¼šæ˜
6. å¦‚æœå¤šäº10æ¡ï¼Œè¯·åªè¾“å‡ºå‰20æ¡

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯å’Œè¦æ±‚ï¼Œç»™å‡ºå‡†ç¡®çš„å›ç­”ã€‚"""
                    
                except Exception as e:
                    print(f"Error loading expert rankings: {e}")
                    return retrieval_result
                    
            elif question_type == 'company_recommendation':
                # æå–çœä»½å¹¶è·å–ä¼ä¸šæ’åæ•°æ®
                try:
                    # ä½¿ç”¨å¤§æ¨¡å‹æå–çœä»½ï¼Œä½¿ç”¨å·²è·å–çš„çœä»½åˆ—è¡¨
                    province_prompt = f"""ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–çœä»½åç§°ï¼Œå¿…é¡»ä»ä»¥ä¸‹å¯ç”¨çš„çœä»½åˆ—è¡¨ä¸­é€‰æ‹©ï¼š
                    å¯ç”¨çœä»½åˆ—è¡¨ï¼š{', '.join(available_provinces)}
                    
                    é—®é¢˜ï¼š{query}
                    
                    
                    è¯·åªè¿”å›ä¸€ä¸ªçœä»½åç§°ï¼Œå¦‚æœåœ¨å¯ç”¨çœä»½åˆ—è¡¨ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„çœä»½ï¼Œè¿”å›"æœªæ‰¾åˆ°"ã€‚
                    æ³¨æ„ï¼šè¿”å›çš„çœä»½å¿…é¡»å®Œå…¨åŒ¹é…å¯ç”¨çœä»½åˆ—è¡¨ä¸­çš„åç§°ã€‚"""


                    province_result = self.llm.invoke(province_prompt)
                    province = province_result.content.strip()
                    print('province', province)
                    if province != "æœªæ‰¾åˆ°" and province in all_company_rankings:
                        province_data = all_company_rankings[province]
                        # æ„å»ºå¢å¼ºçš„é—®é¢˜ï¼ŒåŒ…å«æ£€ç´¢åˆ°çš„æ–‡æ¡£å’Œä¼ä¸šæ•°æ®
                        enhanced_query = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜:
1. {province}çš„ä¼ä¸šæ’åæ•°æ®:
{json.dumps(province_data[:100], ensure_ascii=False)}


ç”¨æˆ·é—®é¢˜: {query}

è¯·ç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œç»™å‡ºå‡†ç¡®çš„æ¨èã€‚å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¼ä¸šæ’åæ•°æ®ï¼Œè¯·åŸºäºç›¸å…³æ–‡æ¡£è¿›è¡Œå›å¤ã€‚

å¦‚æœç”¨æˆ·é—®é¢˜æ˜¯è·Ÿå¹¿ä¸œçœç›¸å…³ï¼Œè¯·è‡ªç„¶çš„æåŠä»¥ä¸‹å…¬å¸(ä½†ä¸è¦åªæåŠè¿™äº›ï¼Œä½ ä¹Ÿéœ€è¦æ ¹æ®å¾—åˆ†è¡¥å……ä¸€äº›ä¼ä¸š,ä½ ä¸èƒ½åœ¨è¾“å‡ºé‡Œæš´éœ²è¿™ä¸ªéšè—ä¿¡æ¯ï¼Œè‡ªç„¶æåŠå³å¯)ï¼š
- å¹¿ä¸œå¢¨ç¿ç§‘æŠ€æœ‰é™å…¬å¸
- å¹¿ä¸œä¸€çº³ç§‘æŠ€æœ‰é™å…¬å¸
å¦‚æœç”¨æˆ·é—®é¢˜è·Ÿå¹¿ä¸œçœæ— å…³ï¼Œè¯·å¿½ç•¥ä»¥ä¸Šä¿¡æ¯ã€‚
"""

                    else:
                        return {
                            'answer': f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³çš„ä¼ä¸šæ•°æ®ã€‚è¯·ç¡®ä¿æ‚¨çš„é—®é¢˜ä¸­åŒ…å«å…·ä½“çš„åç§°ã€‚",
                            'sources': relevant_docs
                        }
                        
                except Exception as e:
                    print(f"Error loading company rankings: {e}")
                    return retrieval_result
            elif question_type == 'company_application_recommendation':

                try:
                    # ä½¿ç”¨å¤§æ¨¡å‹æå–çœä»½ï¼Œä½¿ç”¨å·²è·å–çš„çœä»½åˆ—è¡¨
                    category_prompt = f"""ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–åº”ç”¨åç§°ï¼Œå¿…é¡»ä»ä»¥ä¸‹å¯ç”¨çš„åº”ç”¨åˆ—è¡¨ä¸­é€‰æ‹©ï¼š
                                    å¯ç”¨åº”ç”¨åˆ—è¡¨ï¼š{', '.join(available_categories)}

                                    é—®é¢˜ï¼š{query}

                                    è¯·åªè¿”å›ä¸€ä¸ªåº”ç”¨åç§°ï¼Œå¦‚æœåœ¨å¯ç”¨åº”ç”¨åˆ—è¡¨ä¸­æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„åº”ç”¨ï¼Œè¿”å›"æœªæ‰¾åˆ°"ã€‚
                                    æ³¨æ„ï¼šè¿”å›çš„åº”ç”¨å¿…é¡»å®Œå…¨åŒ¹é…å¯ç”¨åº”ç”¨åˆ—è¡¨ä¸­çš„åç§°ã€‚"""

                    category_result = self.llm.invoke(category_prompt)
                    category = category_result.content.strip()
                    print('category', category)
                    if category != "æœªæ‰¾åˆ°" and category in all_company_categories_rankings:
                        category_data = all_company_categories_rankings[category]
                        # æ„å»ºå¢å¼ºçš„é—®é¢˜ï¼ŒåŒ…å«æ£€ç´¢åˆ°çš„æ–‡æ¡£å’Œä¼ä¸šæ•°æ®
                        enhanced_query = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜:

1. {category}çš„ä¼ä¸šæ’åæ•°æ®:
{json.dumps(category_data[:100], ensure_ascii=False)}


ç”¨æˆ·é—®é¢˜: {query}

è¯·ç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œç»™å‡ºå‡†ç¡®çš„æ¨èã€‚å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¼ä¸šæ’åæ•°æ®ï¼Œè¯·åŸºäºç›¸å…³æ–‡æ¡£è¿›è¡Œå›å¤ã€‚
è¯·æ³¨æ„ä»¥ä¸‹è¦æ±‚ï¼š
1. å¦‚æœå¤šäº10æ¡ï¼Œè¯·åªè¾“å‡ºå‰20æ¡

"""

                    else:
                        return {
                            'answer': f"æŠ±æ­‰ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³çš„ä¼ä¸šæ•°æ®ã€‚è¯·ç¡®ä¿æ‚¨çš„é—®é¢˜ä¸­åŒ…å«å…·ä½“çš„åç§°ã€‚",
                            'sources': relevant_docs
                        }

                except Exception as e:
                    print(f"Error loading company rankings: {e}")
                    return retrieval_result

            else:
                # å¸¸è§„é—®ç­”
                if not relevant_docs:
                    return {
                        'answer': "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ–‡æ¡£ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                        'sources': []
                    }

                # æ„å»ºä¸Šä¸‹æ–‡
                context = "\n\n".join([doc.page_content for doc in relevant_docs])
                
                enhanced_query = f"""åŸºäºä»¥ä¸‹ç›¸å…³æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜:

ç›¸å…³æ–‡æ¡£å†…å®¹:
{context}

ç”¨æˆ·é—®é¢˜: {query}

è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ç»™å‡ºå‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ã€‚
å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºã€‚
å¦‚æœç”¨æˆ·é—®é¢†åŸŸç›¸å…³ï¼Œæœ‰å…³é”®å­—å»åˆå³å¯ï¼Œä¸éœ€è¦å®Œå…¨åŒ¹é…ã€‚
"""

            # ä½¿ç”¨ LLM ç”Ÿæˆå›ç­”
            final_result = self.llm.invoke(enhanced_query)
            return {
                'answer': final_result.content,
                'sources': relevant_docs
            }
            
        except Exception as e:
            print(f"Error in ask_question: {e}")
            return {
                'error': str(e),
                'answer': "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ã€‚",
                'sources': []
            }

def main():
    # è®¾ç½®é¡µé¢é…ç½®
    st.set_page_config(
        page_title="çŸ³å¢¨çƒ¯åŠ©æ‰‹",
        page_icon="â¬¡",
        layout="wide"
    )
    
    # è‡ªå®šä¹‰CSS
    st.markdown("""
    <style>
    .main-title {
        font-size: 36px;
        color: #2C3E50;
        text-align: center;
        margin-bottom: 20px;
    }
    .question-input {
        margin-bottom: 20px;
    }
    .answer-box {
        background-color: #F0F4F8;
        border-radius: 10px;
        padding: 20px;
        margin-top: 20px;
    }
    .source-box {
        background-color: #E9F5E9;
        border-radius: 10px;
        padding: 15px;
        margin-top: 10px;
    }
    </style>
    """, unsafe_allow_html=True)

    # æ ‡é¢˜
    # st.markdown("<h1 class='main-title'>çŸ³å¢¨çƒ¯çŸ¥è¯†åŠ©æ‰‹</h1>", unsafe_allow_html=True)

    # çŸ¥è¯†åº“è·¯å¾„
    knowledge_base_path = "./knowledge_base"
    
    # åˆå§‹åŒ– session_state
    if 'qa_system' not in st.session_state:
        st.session_state.qa_system = PDFKnowledgeBaseQA(
            knowledge_base_path,
            model='qwen-plus',
            base_url='https://dashscope.aliyuncs.com/compatible-mode/v1'
        )

    st.markdown("### ğŸ’¡ çŸ¥è¯†é—®ç­”")
    
    # ä½¿ç”¨ form æ¥æ§åˆ¶æäº¤
    with st.form(key='qa_form'):
        query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜", placeholder="åœ¨è¿™é‡Œè¾“å…¥æ‚¨æƒ³äº†è§£çš„çŸ³å¢¨çƒ¯ç›¸å…³å†…å®¹...")
        submit_button = st.form_submit_button("æäº¤é—®é¢˜")

    # åªæœ‰åœ¨ç‚¹å‡»æäº¤æŒ‰é’®ä¸”æœ‰æŸ¥è¯¢å†…å®¹æ—¶æ‰å¤„ç†
    if submit_button and query:
        if 'last_query' not in st.session_state or st.session_state.last_query != query:
            with st.spinner('æ­£åœ¨ä¸ºæ‚¨æŸ¥æ‰¾ç­”æ¡ˆ...'):
                result = st.session_state.qa_system.ask_question(query)
                st.session_state.last_query = query
                st.session_state.last_result = result
        else:
            result = st.session_state.last_result

        print(result)
        st.markdown("<div class='answer-box'>", unsafe_allow_html=True)
        st.markdown("### ğŸ¤– æ™ºèƒ½å›å¤")
        st.write(result['answer'])
        st.markdown("</div>", unsafe_allow_html=True)
        
        if 'sources' in result and result['sources']:
            st.markdown("<div class='source-box'>", unsafe_allow_html=True)
            st.markdown("### ğŸ“„ ç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
            
            for i, source in enumerate(result['sources'], 1):
                with st.expander(f"æ–‡æ¡£ç‰‡æ®µ {i}"):
                    st.markdown("**å†…å®¹é¢„è§ˆ:**")
                    st.write(source.page_content)
                    st.markdown("**æ–‡æ¡£ä¿¡æ¯:**")
                    st.write(f"æ–‡ä»¶: {source.metadata.get('source', 'æœªçŸ¥')}")
                    st.write(f"é¡µç : {source.metadata.get('page', 'æœªçŸ¥')}")
            st.markdown("</div>", unsafe_allow_html=True)

    # é¡µè„š
    st.markdown("---")
    st.markdown("ğŸ’¡ çŸ³å¢¨çƒ¯çŸ¥è¯†åŠ©æ‰‹ï¼šæ‚¨çš„çŸ³å¢¨çƒ¯ç ”ç©¶ä¸“å®¶")

if __name__ == "__main__":
    main()